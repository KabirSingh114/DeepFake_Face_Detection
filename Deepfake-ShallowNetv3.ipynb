{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = '1'\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "K.set_session(sess)\n",
    "from keras import regularizers\n",
    "from keras.regularizers import l2\n",
    "from keras.layers import Input, Dense, Flatten, GlobalAveragePooling2D, Activation, Conv2D, MaxPooling2D, BatchNormalization, Lambda, Dropout\n",
    "from keras.layers import SeparableConv2D, Add, Convolution2D, concatenate, Layer, ReLU, DepthwiseConv2D, Reshape, Multiply, InputSpec\n",
    "from keras.models import Model, load_model, model_from_json\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from keras.utils import to_categorical\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import brentq\n",
    "from scipy.interpolate import interp1d\n",
    "import glob\n",
    "from PIL import Image\n",
    "from tqdm import tqdm, trange\n",
    "import random\n",
    "from keras.applications import Xception, ResNet152\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classes = 2  # number of classes\n",
    "img_width, img_height = 64, 64  # change based on the shape/structure of your images\n",
    "batch_size = 32  # try 4, 8, 16, 32, 64, 128, 256 dependent on CPU/GPU memory capacity (powers of 2 values).\n",
    "nb_epoch = 300  # number of iteration the algorithm gets trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bgr(img):\n",
    "    return cv2.cvtColor(img, cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = '/mnt/a/fakedata/deepfake/train'\n",
    "validation_dir = '/mnt/a/fakedata/deepfake/val'\n",
    "test50_dir = '/mnt/a/fakedata/deepfake/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/www/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "Loaded Model from disk\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 64, 64, 3)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 60, 60, 32)        2432      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 60, 60, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 60, 60, 32)        1056      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 60, 60, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 60, 60, 32)        1056      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 60, 60, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 30, 30, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 28, 28, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 28, 28, 64)        4160      \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 28, 28, 64)        4160      \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 12, 12, 128)       73856     \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 12, 12, 32)        4128      \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 12, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              4719616   \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 2050      \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 4,835,106\n",
      "Trainable params: 4,833,058\n",
      "Non-trainable params: 2,048\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# ShallowNet V3\n",
    "img_input = Input(shape=(img_height, img_width, 3))\n",
    "\n",
    "# block 1\n",
    "x = Conv2D(32, (5, 5), padding='valid', kernel_regularizer=regularizers.l2(0.0001))(img_input)\n",
    "x = Activation('relu')(x)\n",
    "# x = Dropout(0.25)(x)\n",
    "x = Conv2D(32, (1, 1), kernel_regularizer=regularizers.l2(0.0001))(x)\n",
    "x = Activation('relu')(x)\n",
    "# x = Dropout(0.25)(x)\n",
    "x = Conv2D(32, (1, 1), kernel_regularizer=regularizers.l2(0.0001))(x)\n",
    "x = Activation('relu')(x)\n",
    "x = MaxPooling2D((2, 2))(x)\n",
    "# x = Dropout(0.25)(x)\n",
    "\n",
    "# block 2\n",
    "x = Conv2D(64, (3, 3), padding='valid', kernel_regularizer=regularizers.l2(0.0001))(x)\n",
    "x = Activation('relu')(x)\n",
    "# x = Dropout(0.25)(x)\n",
    "x = Conv2D(64, (1, 1), kernel_regularizer=regularizers.l2(0.0001))(x)\n",
    "x = Activation('relu')(x)\n",
    "# x = Dropout(0.25)(x)\n",
    "x = Conv2D(64, (1, 1), kernel_regularizer=regularizers.l2(0.0001))(x)\n",
    "x = Activation('relu')(x)\n",
    "x = MaxPooling2D((2, 2))(x)\n",
    "# x = Dropout(0.25)(x)\n",
    "\n",
    "# block 3\n",
    "x = Conv2D(128, (3, 3), padding='valid', kernel_regularizer=regularizers.l2(0.0001))(x)\n",
    "x = Activation('relu')(x)\n",
    "# x = Dropout(0.25)(x)\n",
    "x = Conv2D(32, (1, 1), kernel_regularizer=regularizers.l2(0.0001))(x)\n",
    "x = Activation('relu')(x)\n",
    "# x = Dropout(0.25)(x)\n",
    "\n",
    "# block 4\n",
    "x_flatten = Flatten()(x)\n",
    "x_de = Dense(1024, kernel_regularizer=regularizers.l2(0.0001))(x_flatten)\n",
    "x_rl = Activation('relu')(x_de)\n",
    "x_bn = BatchNormalization()(x_rl)\n",
    "x_dp = Dropout(0.25)(x_bn)\n",
    "\n",
    "x_sig = Dense(2, activation=None)(x_bn)\n",
    "out = Activation('sigmoid')(x_sig)\n",
    "\n",
    "model = Model(img_input, out)\n",
    "print(\"Loaded Model from disk\")\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=Adam(),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(len(model.trainable_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 60000 images belonging to 2 classes.\n",
      "Found 18000 images belonging to 2 classes.\n",
      "Found 20000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1./255, preprocessing_function=bgr)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255, preprocessing_function=bgr)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(train_dir,\n",
    "                                                        target_size=(img_height, img_width),\n",
    "                                                        batch_size=batch_size,\n",
    "                                                        shuffle=True,\n",
    "                                                        class_mode='categorical')\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(validation_dir,\n",
    "                                                        target_size=(img_height, img_width),\n",
    "                                                        batch_size=batch_size,\n",
    "                                                        shuffle=False,\n",
    "                                                        class_mode='categorical')\n",
    "\n",
    "test50_generator = test_datagen.flow_from_directory(test50_dir,\n",
    "                                                  target_size=(img_height, img_width),\n",
    "                                                  batch_size=batch_size,\n",
    "                                                  shuffle=False,\n",
    "                                                  class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# callback_list = [EarlyStopping(monitor='val_accuracy', patience=10),\n",
    "#                  ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3)]\n",
    "# history = model.fit_generator(train_generator,\n",
    "#                             steps_per_epoch=200,\n",
    "#                             epochs=100,\n",
    "#                             validation_data=validation_generator,\n",
    "#                             validation_steps=len(validation_generator),\n",
    "#                             callbacks=callback_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save('/home/www/fake_detection/model/deepfake_shallownet.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = load_model('/home/www/fake_detection/model/deepfake_shallownet.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output = model.predict_generator(test50_generator, steps=len(test50_generator), verbose=1)\n",
    "# np.set_printoptions(formatter={'float': lambda x: \"{0:0.3f}\".format(x)})\n",
    "# print(test50_generator.class_indices)\n",
    "# print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_score50 = []\n",
    "# output_class50 = []\n",
    "# answer_class50 = []\n",
    "# answer_class50_1 =[]\n",
    "\n",
    "# for i in trange(len(test50_generator)):\n",
    "#     output50 = model.predict_on_batch(test50_generator[i][0])\n",
    "#     output_score50.append(output50)\n",
    "#     answer_class50.append(test50_generator[i][1])\n",
    "    \n",
    "# output_score50 = np.concatenate(output_score50)\n",
    "# answer_class50 = np.concatenate(answer_class50)\n",
    "\n",
    "# output_class50 = np.argmax(output_score50, axis=1)\n",
    "# answer_class50_1 = np.argmax(answer_class50, axis=1)\n",
    "\n",
    "# print(output_class50)\n",
    "# print(answer_class50_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# cm50 = confusion_matrix(answer_class50_1, output_class50)\n",
    "# report50 = classification_report(answer_class50_1, output_class50)\n",
    "\n",
    "# recall50 = cm50[0][0] / (cm50[0][0] + cm50[0][1])\n",
    "# fallout50 = cm50[1][0] / (cm50[1][0] + cm50[1][1])\n",
    "\n",
    "# fpr50, tpr50, thresholds50 = roc_curve(answer_class50_1, output_score50[:, 1], pos_label=1.)\n",
    "# eer50 = brentq(lambda x : 1. - x - interp1d(fpr50, tpr50)(x), 0., 1.)\n",
    "# thresh50 = interp1d(fpr50, thresholds50)(eer50)\n",
    "\n",
    "# print(report50)\n",
    "# print(cm50)\n",
    "# print(\"AUROC: %f\" %(roc_auc_score(answer_class50_1, output_score50[:, 1])))\n",
    "# print(thresh50)\n",
    "# print('test_acc: ', len(output_class50[np.equal(output_class50, answer_class50_1)]) / len(output_class50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cutout(img):\n",
    "    \"\"\"\n",
    "    # Function: RandomCrop (ZeroPadded (4, 4)) + random occulusion image\n",
    "    # Arguments:\n",
    "        img: image\n",
    "    # Returns:\n",
    "        img\n",
    "    \"\"\"\n",
    "    img = bgr(img)\n",
    "    height = img.shape[0]\n",
    "    width = img.shape[1]\n",
    "    channels = img.shape[2]\n",
    "    MAX_CUTS = 3 # chance to get more cuts\n",
    "    MAX_LENGTH_MUTIPLIER = 5 # chance to get larger cuts\n",
    "    # 16 for cifar10, 8 for cifar100\n",
    "    \n",
    "    # Zero-padded (4, 4)\n",
    "#     img = np.pad(img, ((4,4),(4,4),(0,0)), mode='constant', constant_values=(0))\n",
    "    \n",
    "#     # random-crop 64x64\n",
    "#     dy, dx = height, width\n",
    "#     x = np.random.randint(0, width - dx + 1)\n",
    "#     y = np.random.randint(0, height - dy + 1)\n",
    "#     img = img[y:(y+dy), x:(x+dx)]\n",
    "    \n",
    "#     mean norm\n",
    "#     mean = img.mean(keepdims=True)\n",
    "#     img -= mean\n",
    "\n",
    "    img *= 1./255\n",
    "    \n",
    "    mask = np.ones((height, width, channels), dtype=np.float32)\n",
    "    nb_cuts = np.random.randint(0, MAX_CUTS + 1)\n",
    "    \n",
    "    # cutout\n",
    "    for i in range(nb_cuts):\n",
    "        y = np.random.randint(height)\n",
    "        x = np.random.randint(width)\n",
    "        length = 4 * np.random.randint(1, MAX_LENGTH_MUTIPLIER+1)\n",
    "        \n",
    "        y1 = np.clip(y-length//2, 0, height)\n",
    "        y2 = np.clip(y+length//2, 0, height)\n",
    "        x1 = np.clip(x-length//2, 0, width)\n",
    "        x2 = np.clip(x+length//2, 0, width)\n",
    "        \n",
    "        mask[y1:y2, x1:x2, :] = 0.\n",
    "    \n",
    "    img = img * mask\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU6(Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__(name=\"ReLU6\")\n",
    "        self.relu6 = ReLU(max_value=6, name=\"ReLU6\")\n",
    "\n",
    "    def call(self, input):\n",
    "        return self.relu6(input)\n",
    "\n",
    "\n",
    "class HardSigmoid(Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.relu6 = ReLU6()\n",
    "\n",
    "    def call(self, input):\n",
    "        return self.relu6(input + 3.0) / 6.0\n",
    "\n",
    "\n",
    "class HardSwish(Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hard_sigmoid = HardSigmoid()\n",
    "\n",
    "    def call(self, input):\n",
    "        return input * self.hard_sigmoid(input)\n",
    "    \n",
    "class Attention(Layer):\n",
    "    def __init__(self, ch, **kwargs):\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "        self.channels = ch\n",
    "        self.filters_f_g = self.channels // 8\n",
    "        self.filters_h = self.channels\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        kernel_shape_f_g = (1, 1) + (self.channels, self.filters_f_g)\n",
    "        print(kernel_shape_f_g)\n",
    "        kernel_shape_h = (1, 1) + (self.channels, self.filters_h)\n",
    "\n",
    "        # Create a trainable weight variable for this layer:\n",
    "        self.gamma = self.add_weight(name='gamma', shape=[1], initializer='zeros', trainable=True)\n",
    "        self.kernel_f = self.add_weight(shape=kernel_shape_f_g,\n",
    "                                        initializer='glorot_uniform',\n",
    "                                        name='kernel_f')\n",
    "        self.kernel_g = self.add_weight(shape=kernel_shape_f_g,\n",
    "                                        initializer='glorot_uniform',\n",
    "                                        name='kernel_g')\n",
    "        self.kernel_h = self.add_weight(shape=kernel_shape_h,\n",
    "                                        initializer='glorot_uniform',\n",
    "                                        name='kernel_h')\n",
    "        self.bias_f = self.add_weight(shape=(self.filters_f_g,),\n",
    "                                      initializer='zeros',\n",
    "                                      name='bias_F')\n",
    "        self.bias_g = self.add_weight(shape=(self.filters_f_g,),\n",
    "                                      initializer='zeros',\n",
    "                                      name='bias_g')\n",
    "        self.bias_h = self.add_weight(shape=(self.filters_h,),\n",
    "                                      initializer='zeros',\n",
    "                                      name='bias_h')\n",
    "        super(Attention, self).build(input_shape)\n",
    "        # Set input spec.\n",
    "        self.input_spec = InputSpec(ndim=4,\n",
    "                                    axes={3: input_shape[-1]})\n",
    "        self.built = True\n",
    "\n",
    "\n",
    "    def call(self, x):\n",
    "        def hw_flatten(x):\n",
    "            return K.reshape(x, shape=[K.shape(x)[0], K.shape(x)[1]*K.shape(x)[2], K.shape(x)[-1]])\n",
    "\n",
    "        f = K.conv2d(x,\n",
    "                     kernel=self.kernel_f,\n",
    "                     strides=(1, 1), padding='same')  # [bs, h, w, c']\n",
    "        f = K.bias_add(f, self.bias_f)\n",
    "        g = K.conv2d(x,\n",
    "                     kernel=self.kernel_g,\n",
    "                     strides=(1, 1), padding='same')  # [bs, h, w, c']\n",
    "        g = K.bias_add(g, self.bias_g)\n",
    "        h = K.conv2d(x,\n",
    "                     kernel=self.kernel_h,\n",
    "                     strides=(1, 1), padding='same')  # [bs, h, w, c]\n",
    "        h = K.bias_add(h, self.bias_h)\n",
    "\n",
    "        s = tf.matmul(hw_flatten(g), hw_flatten(f), transpose_b=True)  # # [bs, N, N]\n",
    "\n",
    "        beta = K.softmax(s, axis=-1)  # attention map\n",
    "\n",
    "        o = K.batch_dot(beta, hw_flatten(h))  # [bs, N, C]\n",
    "\n",
    "        o = K.reshape(o, shape=K.shape(x))  # [bs, h, w, C]\n",
    "        x = self.gamma * o + x\n",
    "\n",
    "        return x\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n",
      "Found 18000 images belonging to 2 classes.\n",
      "Found 20000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "ft_dir = '/mnt/a/fakedata/deepfake/finetune'\n",
    "train_gen_aug = ImageDataGenerator(shear_range=0, \n",
    "                               zoom_range=0.2,\n",
    "                               rotation_range=0.2,\n",
    "                               width_shift_range=2, \n",
    "                               height_shift_range=2,\n",
    "                               horizontal_flip=True,\n",
    "                               zca_whitening=False,\n",
    "                               fill_mode='nearest',\n",
    "                               preprocessing_function=cutout)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255, preprocessing_function=bgr)\n",
    "\n",
    "ft_gen = train_gen_aug.flow_from_directory(ft_dir,\n",
    "                                              target_size=(img_height, img_width),\n",
    "                                              batch_size=batch_size,\n",
    "                                              shuffle=True,\n",
    "                                              class_mode='categorical')\n",
    "\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(validation_dir,\n",
    "                                                        target_size=(img_height, img_width),\n",
    "                                                        batch_size=batch_size,\n",
    "                                                        shuffle=False,\n",
    "                                                        class_mode='categorical')\n",
    "\n",
    "test50_generator = test_datagen.flow_from_directory(test50_dir,\n",
    "                                                  target_size=(img_height, img_width),\n",
    "                                                  batch_size=batch_size,\n",
    "                                                  shuffle=False,\n",
    "                                                  class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/www/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/www/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "(1, 1, 32, 4)\n",
      "(1, 1, 64, 8)\n",
      "(1, 1, 128, 16)\n",
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 64, 64, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model_2 (Model)                 (None, 12, 12, 32)   109344      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 12, 12, 1024) 32768       model_2[1][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 12, 12, 1024) 4096        conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "hard_swish_1 (HardSwish)        (None, 12, 12, 1024) 0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv2d_1 (DepthwiseCo (None, 6, 6, 1024)   9216        hard_swish_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 6, 6, 1024)   4096        depthwise_conv2d_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_1 (Glo (None, 1024)         0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 1, 1, 1024)   0           global_average_pooling2d_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 1, 1, 256)    262144      reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 1, 1, 256)    0           conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 1, 1, 1024)   262144      activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "hard_sigmoid_2 (HardSigmoid)    (None, 1, 1, 1024)   0           conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "multiply_1 (Multiply)           (None, 12, 12, 1024) 0           hard_swish_1[0][0]               \n",
      "                                                                 hard_sigmoid_2[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "hard_swish_2 (HardSwish)        (None, 12, 12, 1024) 0           multiply_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 12, 12, 256)  262144      hard_swish_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 12, 12, 256)  1024        conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 12, 12, 1024) 262144      batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 12, 12, 1024) 4096        conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "hard_swish_3 (HardSwish)        (None, 12, 12, 1024) 0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv2d_2 (DepthwiseCo (None, 12, 12, 1024) 25600       hard_swish_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 12, 12, 1024) 4096        depthwise_conv2d_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_2 (Glo (None, 1024)         0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "reshape_2 (Reshape)             (None, 1, 1, 1024)   0           global_average_pooling2d_2[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 1, 1, 256)    262144      reshape_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 1, 1, 256)    0           conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 1, 1, 1024)   262144      activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "hard_sigmoid_5 (HardSigmoid)    (None, 1, 1, 1024)   0           conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "multiply_2 (Multiply)           (None, 12, 12, 1024) 0           hard_swish_3[0][0]               \n",
      "                                                                 hard_sigmoid_5[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "hard_swish_4 (HardSwish)        (None, 12, 12, 1024) 0           multiply_2[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 12, 12, 256)  262144      hard_swish_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 12, 12, 256)  1024        conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 12, 12, 256)  0           batch_normalization_4[0][0]      \n",
      "                                                                 batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 12, 12, 1024) 262144      add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 12, 12, 1024) 4096        conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "hard_swish_5 (HardSwish)        (None, 12, 12, 1024) 0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv2d_3 (DepthwiseCo (None, 12, 12, 1024) 25600       hard_swish_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 12, 12, 1024) 4096        depthwise_conv2d_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_3 (Glo (None, 1024)         0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "reshape_3 (Reshape)             (None, 1, 1, 1024)   0           global_average_pooling2d_3[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 1, 1, 256)    262144      reshape_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 1, 1, 256)    0           conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 1, 1, 1024)   262144      activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "hard_sigmoid_8 (HardSigmoid)    (None, 1, 1, 1024)   0           conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "multiply_3 (Multiply)           (None, 12, 12, 1024) 0           hard_swish_5[0][0]               \n",
      "                                                                 hard_sigmoid_8[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "hard_swish_6 (HardSwish)        (None, 12, 12, 1024) 0           multiply_3[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 12, 12, 256)  262144      hard_swish_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 12, 12, 256)  1024        conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 12, 12, 256)  0           add_1[0][0]                      \n",
      "                                                                 batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 12, 12, 1024) 262144      add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 12, 12, 1024) 4096        conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "hard_swish_7 (HardSwish)        (None, 12, 12, 1024) 0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv2d_4 (DepthwiseCo (None, 12, 12, 1024) 25600       hard_swish_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 12, 12, 1024) 4096        depthwise_conv2d_4[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_1 (SeparableCo (None, 32, 32, 32)   123         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_4 (Glo (None, 1024)         0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 32, 32, 32)   128         separable_conv2d_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "reshape_4 (Reshape)             (None, 1, 1, 1024)   0           global_average_pooling2d_4[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 32, 32, 32)   0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 1, 1, 256)    262144      reshape_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "attention_1 (Attention)         (None, 32, 32, 32)   1321        activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 1, 1, 256)    0           conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_2 (SeparableCo (None, 16, 16, 64)   2336        attention_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 1, 1, 1024)   262144      activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 16, 16, 64)   256         separable_conv2d_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "hard_sigmoid_11 (HardSigmoid)   (None, 1, 1, 1024)   0           conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 16, 16, 64)   0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "multiply_4 (Multiply)           (None, 12, 12, 1024) 0           hard_swish_7[0][0]               \n",
      "                                                                 hard_sigmoid_11[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_2 (Attention)         (None, 16, 16, 64)   5201        activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "hard_swish_8 (HardSwish)        (None, 12, 12, 1024) 0           multiply_4[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_3 (SeparableCo (None, 8, 8, 128)    8768        attention_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 12, 12, 256)  262144      hard_swish_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 8, 8, 128)    512         separable_conv2d_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 12, 12, 256)  1024        conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 8, 8, 128)    0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 12, 12, 256)  0           add_2[0][0]                      \n",
      "                                                                 batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "attention_3 (Attention)         (None, 8, 8, 128)    20641       activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 12, 12, 576)  147456      add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 8, 8, 576)    73728       attention_3[0][0]                \n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_normalization_14 (BatchNo (None, 12, 12, 576)  2304        conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 8, 8, 576)    2304        conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "hard_swish_9 (HardSwish)        (None, 12, 12, 576)  0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 8, 8, 576)    0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_5 (Glo (None, 576)          0           hard_swish_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_6 (Glo (None, 576)          0           activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 576)          0           global_average_pooling2d_5[0][0] \n",
      "                                                                 global_average_pooling2d_6[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 2)            1154        add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 2)            0           dense_3[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 4,463,384\n",
      "Trainable params: 4,442,200\n",
      "Non-trainable params: 21,184\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_ft = load_model('/home/www/fake_detection/model/deepfake_shallownet.h5')\n",
    "for i in range(7):\n",
    "    model_ft.layers.pop()\n",
    "im_in = Input(shape=(img_width, img_height, 3))\n",
    "\n",
    "base_model = Model(img_input, x)\n",
    "base_model.set_weights(model_ft.get_weights())\n",
    "# for i in range(len(base_model.layers) - 0):\n",
    "#     base_model.layers[i].trainable = False\n",
    "    \n",
    "x1 = base_model(im_in) # (12, 12, 32)\n",
    "########### Mobilenet block bneck 3x3 (32 --> 128) #################\n",
    "expand1 = Conv2D(1024, kernel_size=1, strides=1, kernel_regularizer=l2(1e-5), use_bias=False)(x1)\n",
    "expand1 = BatchNormalization()(expand1)\n",
    "expand1 = HardSwish()(expand1)\n",
    "dw1 = DepthwiseConv2D(kernel_size=(3,3), strides=(2,2), padding='same', depthwise_regularizer=l2(1e-5), use_bias=False)(expand1)\n",
    "dw1 = BatchNormalization()(dw1)\n",
    "se_gap1 = GlobalAveragePooling2D()(dw1)\n",
    "se_gap1 = Reshape([1, 1, -1])(se_gap1)\n",
    "se1 = Conv2D(256, kernel_size=1, strides=1, padding='valid', kernel_regularizer=l2(1e-5), use_bias=False)(se_gap1)\n",
    "se1 = Activation('relu')(se1)\n",
    "se1 = Conv2D(1024, kernel_size=1, strides=1, padding='valid', kernel_regularizer=l2(1e-5), use_bias=False)(se1)\n",
    "se1 = HardSigmoid()(se1)\n",
    "se1 = Multiply()([expand1, se1])\n",
    "project1 = HardSwish()(se1)\n",
    "project1 = Conv2D(256, kernel_size=(1, 1), padding='valid', kernel_regularizer=l2(1e-5), use_bias=False)(project1)\n",
    "project1 = BatchNormalization()(project1)\n",
    "\n",
    "########### Mobilenet block bneck 5x5 (128 --> 128) #################\n",
    "expand2 = Conv2D(1024, kernel_size=1, strides=1, kernel_regularizer=l2(1e-5), use_bias=False)(project1)\n",
    "expand2 = BatchNormalization()(expand2)\n",
    "expand2 = HardSwish()(expand2)\n",
    "dw2 = DepthwiseConv2D(kernel_size=(5,5), strides=(1,1), padding='same', depthwise_regularizer=l2(1e-5), use_bias=False)(expand2)\n",
    "dw2 = BatchNormalization()(dw2)\n",
    "se_gap2 = GlobalAveragePooling2D()(dw2)\n",
    "se_gap2 = Reshape([1, 1, -1])(se_gap2)\n",
    "se2 = Conv2D(256, kernel_size=1, strides=1, padding='valid', kernel_regularizer=l2(1e-5), use_bias=False)(se_gap2)\n",
    "se2 = Activation('relu')(se2)\n",
    "se2 = Conv2D(1024, kernel_size=1, strides=1, padding='valid', kernel_regularizer=l2(1e-5), use_bias=False)(se2)\n",
    "se2 = HardSigmoid()(se2)\n",
    "se2 = Multiply()([expand2, se2])\n",
    "project2 = HardSwish()(se2)\n",
    "project2 = Conv2D(256, kernel_size=(1, 1), padding='valid', kernel_regularizer=l2(1e-5), use_bias=False)(project2)\n",
    "project2 = BatchNormalization()(project2)\n",
    "project2 = Add()([project1, project2])\n",
    "\n",
    "########### Mobilenet block bneck 5x5 (128 --> 128) #################\n",
    "expand3 = Conv2D(1024, kernel_size=1, strides=1, kernel_regularizer=l2(1e-5), use_bias=False)(project2)\n",
    "expand3 = BatchNormalization()(expand3)\n",
    "expand3 = HardSwish()(expand3)\n",
    "dw3 = DepthwiseConv2D(kernel_size=(5,5), strides=(1,1), padding='same', depthwise_regularizer=l2(1e-5), use_bias=False)(expand3)\n",
    "dw3 = BatchNormalization()(dw3)\n",
    "se_gap3 = GlobalAveragePooling2D()(dw3)\n",
    "se_gap3 = Reshape([1, 1, -1])(se_gap3)\n",
    "se3 = Conv2D(256, kernel_size=1, strides=1, padding='valid', kernel_regularizer=l2(1e-5), use_bias=False)(se_gap3)\n",
    "se3 = Activation('relu')(se3)\n",
    "se3 = Conv2D(1024, kernel_size=1, strides=1, padding='valid', kernel_regularizer=l2(1e-5), use_bias=False)(se3)\n",
    "se3 = HardSigmoid()(se3)\n",
    "se3 = Multiply()([expand3, se3])\n",
    "project3 = HardSwish()(se3)\n",
    "project3 = Conv2D(256, kernel_size=(1, 1), padding='valid', kernel_regularizer=l2(1e-5), use_bias=False)(project3)\n",
    "project3 = BatchNormalization()(project3)\n",
    "project3 = Add()([project2, project3])\n",
    "\n",
    "\n",
    "expand4 = Conv2D(1024, kernel_size=1, strides=1, kernel_regularizer=l2(1e-5), use_bias=False)(project3)\n",
    "expand4 = BatchNormalization()(expand4)\n",
    "expand4 = HardSwish()(expand4)\n",
    "dw4 = DepthwiseConv2D(kernel_size=(5,5), strides=(1,1), padding='same', depthwise_regularizer=l2(1e-5), use_bias=False)(expand4)\n",
    "dw4 = BatchNormalization()(dw4)\n",
    "se_gap4 = GlobalAveragePooling2D()(dw4)\n",
    "se_gap4 = Reshape([1, 1, -1])(se_gap4)\n",
    "se4 = Conv2D(256, kernel_size=1, strides=1, padding='valid', kernel_regularizer=l2(1e-5), use_bias=False)(se_gap4)\n",
    "se4 = Activation('relu')(se4)\n",
    "se4 = Conv2D(1024, kernel_size=1, strides=1, padding='valid', kernel_regularizer=l2(1e-5), use_bias=False)(se4)\n",
    "se4 = HardSigmoid()(se4)\n",
    "se4 = Multiply()([expand4, se4])\n",
    "project4 = HardSwish()(se4)\n",
    "project4 = Conv2D(256, kernel_size=(1, 1), padding='valid', kernel_regularizer=l2(1e-5), use_bias=False)(project4)\n",
    "project4 = BatchNormalization()(project4)\n",
    "project4 = Add()([project3, project4])\n",
    "\n",
    "\n",
    "########## Classification ##########\n",
    "x2 = Conv2D(576, kernel_size=1, strides=1, padding='valid', kernel_regularizer=l2(1e-5), use_bias=False)(project4)\n",
    "x2 = BatchNormalization()(x2)\n",
    "x2 = HardSwish()(x2)\n",
    "x2 = GlobalAveragePooling2D()(x2)\n",
    "\n",
    "\n",
    "######### Image Attention Model #########\n",
    "### Block 1 ###\n",
    "x3 = SeparableConv2D(32, kernel_size=(3, 3), strides=(2,2), padding='same', depthwise_regularizer=l2(1e-5), pointwise_regularizer=l2(1e-5), use_bias=False)(im_in)\n",
    "x3 = BatchNormalization()(x3)\n",
    "x3 = Activation('relu')(x3)\n",
    "x3 = Attention(32)(x3)\n",
    "\n",
    "### Block 2 ###\n",
    "x4 = SeparableConv2D(64, kernel_size=(3, 3), strides=(2,2), padding='same', depthwise_regularizer=l2(1e-5), pointwise_regularizer=l2(1e-5), use_bias=False)(x3)\n",
    "x4 = BatchNormalization()(x4)\n",
    "x4 = Activation('relu')(x4)\n",
    "x4 = Attention(64)(x4)\n",
    "\n",
    "### Block 3 ###\n",
    "x5 = SeparableConv2D(128, kernel_size=(3, 3), strides=(2,2), padding='same', depthwise_regularizer=l2(1e-5), pointwise_regularizer=l2(1e-5), use_bias=False)(x4)\n",
    "x5 = BatchNormalization()(x5)\n",
    "x5 = Activation('relu')(x5)\n",
    "x5 = Attention(128)(x5)\n",
    "\n",
    "### final stage ###\n",
    "x6 = Conv2D(576, kernel_size=1, strides=1, padding='valid', kernel_regularizer=l2(1e-5), use_bias=False)(x5)\n",
    "x6 = BatchNormalization()(x6)\n",
    "x6 = Activation('relu')(x6)\n",
    "x6 = GlobalAveragePooling2D()(x6)\n",
    "# x6 = Reshape([1, 1, -1])(x6)\n",
    "\n",
    "######## final addition #########\n",
    "\n",
    "x2 = Add()([x2, x6])\n",
    "x2 = Dense(2)(x2)\n",
    "x2 = Activation('softmax')(x2)\n",
    "\n",
    "model_top = Model(inputs=im_in, outputs=x2)\n",
    "model_top.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "200/200 [==============================] - 846s 4s/step - loss: 0.7191 - acc: 0.6256 - val_loss: 1.1570 - val_acc: 0.5000\n",
      "Epoch 2/300\n",
      "200/200 [==============================] - 722s 4s/step - loss: 0.6532 - acc: 0.6784 - val_loss: 1.6704 - val_acc: 0.5000\n",
      "Epoch 3/300\n",
      "200/200 [==============================] - 659s 3s/step - loss: 0.6219 - acc: 0.7037 - val_loss: 3.7064 - val_acc: 0.5000\n",
      "Epoch 4/300\n",
      "200/200 [==============================] - 624s 3s/step - loss: 0.6056 - acc: 0.7172 - val_loss: 1.2197 - val_acc: 0.4877\n",
      "Epoch 5/300\n",
      "200/200 [==============================] - 313s 2s/step - loss: 0.5650 - acc: 0.7440 - val_loss: 2.0068 - val_acc: 0.5881\n",
      "Epoch 6/300\n",
      "200/200 [==============================] - 84s 422ms/step - loss: 0.5472 - acc: 0.7487 - val_loss: 0.3814 - val_acc: 0.7043\n",
      "Epoch 7/300\n",
      "200/200 [==============================] - 76s 378ms/step - loss: 0.5297 - acc: 0.7609 - val_loss: 1.2018 - val_acc: 0.6331\n",
      "Epoch 8/300\n",
      "200/200 [==============================] - 80s 402ms/step - loss: 0.5447 - acc: 0.7420 - val_loss: 9.6307 - val_acc: 0.5017\n",
      "Epoch 9/300\n",
      "200/200 [==============================] - 82s 408ms/step - loss: 0.5242 - acc: 0.7627 - val_loss: 1.1098 - val_acc: 0.6501\n",
      "Epoch 10/300\n",
      "200/200 [==============================] - 81s 406ms/step - loss: 0.4965 - acc: 0.7808 - val_loss: 0.6407 - val_acc: 0.8294\n",
      "Epoch 11/300\n",
      "200/200 [==============================] - 77s 385ms/step - loss: 0.5102 - acc: 0.7624 - val_loss: 3.3989 - val_acc: 0.5106\n",
      "Epoch 12/300\n",
      "200/200 [==============================] - 76s 382ms/step - loss: 0.4862 - acc: 0.7823 - val_loss: 0.7954 - val_acc: 0.6917\n",
      "Epoch 13/300\n",
      "200/200 [==============================] - 83s 416ms/step - loss: 0.4738 - acc: 0.7928 - val_loss: 7.2717 - val_acc: 0.5000\n",
      "Epoch 14/300\n",
      "200/200 [==============================] - 83s 413ms/step - loss: 0.4579 - acc: 0.7951 - val_loss: 16.2536 - val_acc: 0.5000\n",
      "Epoch 15/300\n",
      "200/200 [==============================] - 81s 407ms/step - loss: 0.4718 - acc: 0.7824 - val_loss: 2.9618 - val_acc: 0.5724\n",
      "Epoch 16/300\n",
      "200/200 [==============================] - 78s 388ms/step - loss: 0.4569 - acc: 0.8013 - val_loss: 6.4974 - val_acc: 0.5000\n",
      "Epoch 17/300\n",
      "200/200 [==============================] - 85s 423ms/step - loss: 0.4358 - acc: 0.8075 - val_loss: 0.0284 - val_acc: 0.5071\n",
      "Epoch 18/300\n",
      "200/200 [==============================] - 83s 417ms/step - loss: 0.4412 - acc: 0.8079 - val_loss: 16.1836 - val_acc: 0.5019\n",
      "Epoch 19/300\n",
      "200/200 [==============================] - 83s 417ms/step - loss: 0.4217 - acc: 0.8174 - val_loss: 0.8310 - val_acc: 0.8473\n",
      "Epoch 20/300\n",
      "200/200 [==============================] - 77s 383ms/step - loss: 0.4211 - acc: 0.8147 - val_loss: 5.6808 - val_acc: 0.5006\n",
      "Epoch 21/300\n",
      "200/200 [==============================] - 87s 435ms/step - loss: 0.4147 - acc: 0.8160 - val_loss: 2.0554 - val_acc: 0.6554\n",
      "Epoch 22/300\n",
      "200/200 [==============================] - 84s 418ms/step - loss: 0.4334 - acc: 0.8086 - val_loss: 0.4383 - val_acc: 0.7112\n",
      "Epoch 23/300\n",
      "200/200 [==============================] - 82s 410ms/step - loss: 0.4088 - acc: 0.8219 - val_loss: 0.4666 - val_acc: 0.7764\n",
      "Epoch 24/300\n",
      "200/200 [==============================] - 77s 383ms/step - loss: 0.4287 - acc: 0.8193 - val_loss: 18.6783 - val_acc: 0.5000\n",
      "Epoch 25/300\n",
      "200/200 [==============================] - 82s 409ms/step - loss: 0.4040 - acc: 0.8251 - val_loss: 1.2961 - val_acc: 0.7149\n",
      "Epoch 26/300\n",
      "200/200 [==============================] - 83s 414ms/step - loss: 0.4046 - acc: 0.8207 - val_loss: 8.8975 - val_acc: 0.5007\n",
      "Epoch 27/300\n",
      "200/200 [==============================] - 84s 422ms/step - loss: 0.3907 - acc: 0.8384 - val_loss: 1.2332 - val_acc: 0.7540\n",
      "Epoch 28/300\n",
      "200/200 [==============================] - 79s 393ms/step - loss: 0.3922 - acc: 0.8359 - val_loss: 1.6135 - val_acc: 0.6901\n",
      "Epoch 29/300\n",
      "200/200 [==============================] - 85s 424ms/step - loss: 0.3703 - acc: 0.8440 - val_loss: 0.1110 - val_acc: 0.6434\n",
      "Epoch 30/300\n",
      "200/200 [==============================] - 85s 426ms/step - loss: 0.3714 - acc: 0.8419 - val_loss: 0.0672 - val_acc: 0.7814\n",
      "Epoch 31/300\n",
      "200/200 [==============================] - 85s 423ms/step - loss: 0.3607 - acc: 0.8501 - val_loss: 0.0190 - val_acc: 0.5603\n",
      "Epoch 32/300\n",
      "200/200 [==============================] - 77s 386ms/step - loss: 0.3772 - acc: 0.8419 - val_loss: 0.6854 - val_acc: 0.8131\n",
      "Epoch 33/300\n",
      "200/200 [==============================] - 82s 410ms/step - loss: 0.3598 - acc: 0.8504 - val_loss: 2.7623 - val_acc: 0.6347\n",
      "Epoch 34/300\n",
      "200/200 [==============================] - 85s 426ms/step - loss: 0.3645 - acc: 0.8471 - val_loss: 0.0194 - val_acc: 0.5000\n",
      "Epoch 35/300\n",
      "200/200 [==============================] - 85s 425ms/step - loss: 0.3414 - acc: 0.8610 - val_loss: 0.0383 - val_acc: 0.5223\n",
      "Epoch 36/300\n",
      "200/200 [==============================] - 78s 390ms/step - loss: 0.3573 - acc: 0.8521 - val_loss: 0.4635 - val_acc: 0.6800\n",
      "Epoch 37/300\n",
      "200/200 [==============================] - 80s 401ms/step - loss: 0.3498 - acc: 0.8548 - val_loss: 0.3751 - val_acc: 0.8723\n",
      "Epoch 38/300\n",
      "200/200 [==============================] - 85s 426ms/step - loss: 0.3371 - acc: 0.8616 - val_loss: 3.8957 - val_acc: 0.6968\n",
      "Epoch 39/300\n",
      "200/200 [==============================] - 86s 431ms/step - loss: 0.3289 - acc: 0.8677 - val_loss: 18.8833 - val_acc: 0.5006\n",
      "Epoch 40/300\n",
      "200/200 [==============================] - 78s 390ms/step - loss: 0.3436 - acc: 0.8567 - val_loss: 5.8710 - val_acc: 0.5250\n",
      "Epoch 41/300\n",
      "200/200 [==============================] - 81s 403ms/step - loss: 0.3285 - acc: 0.8678 - val_loss: 10.3307 - val_acc: 0.5024\n",
      "Epoch 42/300\n",
      "200/200 [==============================] - 86s 429ms/step - loss: 0.3495 - acc: 0.8557 - val_loss: 0.0625 - val_acc: 0.6223\n",
      "Epoch 43/300\n",
      "200/200 [==============================] - 85s 425ms/step - loss: 0.3345 - acc: 0.8606 - val_loss: 0.0448 - val_acc: 0.7605\n",
      "Epoch 44/300\n",
      "200/200 [==============================] - 81s 405ms/step - loss: 0.3183 - acc: 0.8739 - val_loss: 0.3822 - val_acc: 0.9004\n",
      "Epoch 45/300\n",
      "200/200 [==============================] - 79s 396ms/step - loss: 0.3315 - acc: 0.8654 - val_loss: 0.0201 - val_acc: 0.5051\n",
      "Epoch 46/300\n",
      "200/200 [==============================] - 85s 425ms/step - loss: 0.3396 - acc: 0.8577 - val_loss: 2.9822 - val_acc: 0.6276\n",
      "Epoch 47/300\n",
      "200/200 [==============================] - 84s 422ms/step - loss: 0.3117 - acc: 0.8772 - val_loss: 5.3654 - val_acc: 0.5121\n",
      "Epoch 48/300\n",
      "200/200 [==============================] - 81s 407ms/step - loss: 0.2900 - acc: 0.8868 - val_loss: 0.0202 - val_acc: 0.6148\n",
      "Epoch 49/300\n",
      "200/200 [==============================] - 77s 386ms/step - loss: 0.3337 - acc: 0.8606 - val_loss: 0.1005 - val_acc: 0.8631\n",
      "Epoch 50/300\n",
      "200/200 [==============================] - 84s 419ms/step - loss: 0.3036 - acc: 0.8791 - val_loss: 0.0205 - val_acc: 0.5333\n",
      "Epoch 51/300\n",
      "200/200 [==============================] - 85s 426ms/step - loss: 0.2999 - acc: 0.8781 - val_loss: 4.8984 - val_acc: 0.5054\n",
      "Epoch 52/300\n",
      "200/200 [==============================] - 82s 412ms/step - loss: 0.3325 - acc: 0.8693 - val_loss: 5.3294 - val_acc: 0.5082\n",
      "Epoch 53/300\n",
      "200/200 [==============================] - 77s 385ms/step - loss: 0.2890 - acc: 0.8813 - val_loss: 3.4206 - val_acc: 0.5649\n",
      "Epoch 54/300\n",
      "200/200 [==============================] - 83s 414ms/step - loss: 0.2740 - acc: 0.8925 - val_loss: 0.0190 - val_acc: 0.5106\n",
      "Epoch 55/300\n",
      "200/200 [==============================] - 83s 413ms/step - loss: 0.2676 - acc: 0.8968 - val_loss: 0.0609 - val_acc: 0.8530\n",
      "Epoch 56/300\n",
      "200/200 [==============================] - 83s 414ms/step - loss: 0.2634 - acc: 0.8987 - val_loss: 3.4708 - val_acc: 0.5912\n",
      "Epoch 57/300\n",
      "200/200 [==============================] - 76s 381ms/step - loss: 0.2590 - acc: 0.8988 - val_loss: 0.2310 - val_acc: 0.9317\n",
      "Epoch 58/300\n",
      "200/200 [==============================] - 83s 415ms/step - loss: 0.2475 - acc: 0.9055 - val_loss: 0.0182 - val_acc: 0.5676\n",
      "Epoch 59/300\n",
      "200/200 [==============================] - 84s 418ms/step - loss: 0.2424 - acc: 0.9082 - val_loss: 0.0193 - val_acc: 0.8054\n",
      "Epoch 60/300\n",
      "200/200 [==============================] - 83s 413ms/step - loss: 0.2548 - acc: 0.9009 - val_loss: 0.1065 - val_acc: 0.9243\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/300\n",
      "200/200 [==============================] - 77s 383ms/step - loss: 0.2407 - acc: 0.9097 - val_loss: 2.8573 - val_acc: 0.6552\n",
      "Epoch 62/300\n",
      "200/200 [==============================] - 83s 417ms/step - loss: 0.2572 - acc: 0.8988 - val_loss: 1.0682 - val_acc: 0.8292\n",
      "Epoch 63/300\n",
      "200/200 [==============================] - 84s 422ms/step - loss: 0.2511 - acc: 0.9016 - val_loss: 1.1568 - val_acc: 0.8618\n",
      "Epoch 64/300\n",
      "200/200 [==============================] - 83s 417ms/step - loss: 0.2398 - acc: 0.9057 - val_loss: 3.4510 - val_acc: 0.5900\n",
      "Epoch 65/300\n",
      "200/200 [==============================] - 76s 379ms/step - loss: 0.2480 - acc: 0.9067 - val_loss: 1.8474 - val_acc: 0.7306\n",
      "Epoch 66/300\n",
      "200/200 [==============================] - 81s 404ms/step - loss: 0.2355 - acc: 0.9084 - val_loss: 0.0189 - val_acc: 0.7352\n",
      "Epoch 67/300\n",
      "200/200 [==============================] - 84s 420ms/step - loss: 0.2379 - acc: 0.9063 - val_loss: 1.2814 - val_acc: 0.8323\n",
      "Epoch 68/300\n",
      "200/200 [==============================] - 80s 401ms/step - loss: 0.2338 - acc: 0.9112 - val_loss: 3.1931 - val_acc: 0.6264\n",
      "Epoch 69/300\n",
      "200/200 [==============================] - 77s 387ms/step - loss: 0.2436 - acc: 0.9074 - val_loss: 14.2568 - val_acc: 0.5676\n",
      "Epoch 70/300\n",
      "200/200 [==============================] - 78s 388ms/step - loss: 0.2404 - acc: 0.9074 - val_loss: 2.9086 - val_acc: 0.6662\n",
      "Epoch 71/300\n",
      "200/200 [==============================] - 83s 416ms/step - loss: 0.2321 - acc: 0.9096 - val_loss: 0.4920 - val_acc: 0.9184\n",
      "Epoch 72/300\n",
      "200/200 [==============================] - 84s 422ms/step - loss: 0.2350 - acc: 0.9115 - val_loss: 3.0818 - val_acc: 0.6146\n",
      "Epoch 73/300\n",
      "200/200 [==============================] - 82s 409ms/step - loss: 0.2230 - acc: 0.9128 - val_loss: 0.0192 - val_acc: 0.7854\n",
      "Epoch 74/300\n",
      "200/200 [==============================] - 76s 382ms/step - loss: 0.2220 - acc: 0.9159 - val_loss: 0.0176 - val_acc: 0.6898\n",
      "Epoch 75/300\n",
      "200/200 [==============================] - 83s 416ms/step - loss: 0.2258 - acc: 0.9162 - val_loss: 5.6806 - val_acc: 0.5695\n",
      "Epoch 76/300\n",
      "200/200 [==============================] - 83s 413ms/step - loss: 0.2267 - acc: 0.9151 - val_loss: 0.1164 - val_acc: 0.9145\n",
      "Epoch 77/300\n",
      "200/200 [==============================] - 83s 414ms/step - loss: 0.2129 - acc: 0.9193 - val_loss: 2.3030 - val_acc: 0.6883\n",
      "Epoch 78/300\n",
      "200/200 [==============================] - 77s 385ms/step - loss: 0.2433 - acc: 0.9046 - val_loss: 11.0131 - val_acc: 0.6576\n",
      "Epoch 79/300\n",
      "200/200 [==============================] - 82s 409ms/step - loss: 0.2254 - acc: 0.9151 - val_loss: 0.0364 - val_acc: 0.8891\n",
      "Epoch 80/300\n",
      "200/200 [==============================] - 83s 414ms/step - loss: 0.2141 - acc: 0.9213 - val_loss: 2.0350 - val_acc: 0.7357\n",
      "Epoch 81/300\n",
      "200/200 [==============================] - 83s 416ms/step - loss: 0.2243 - acc: 0.9172 - val_loss: 2.0477 - val_acc: 0.7354\n",
      "Epoch 82/300\n",
      "200/200 [==============================] - 76s 380ms/step - loss: 0.2341 - acc: 0.9092 - val_loss: 4.3140 - val_acc: 0.5659\n",
      "Epoch 83/300\n",
      "200/200 [==============================] - 84s 420ms/step - loss: 0.1979 - acc: 0.9281 - val_loss: 0.1749 - val_acc: 0.9373\n",
      "Epoch 84/300\n",
      "200/200 [==============================] - 83s 417ms/step - loss: 0.2066 - acc: 0.9233 - val_loss: 0.0180 - val_acc: 0.5000\n",
      "Epoch 85/300\n",
      "200/200 [==============================] - 83s 414ms/step - loss: 0.2291 - acc: 0.9134 - val_loss: 0.8052 - val_acc: 0.8855\n",
      "Epoch 86/300\n",
      "200/200 [==============================] - 77s 383ms/step - loss: 0.2068 - acc: 0.9219 - val_loss: 1.0631 - val_acc: 0.9123\n",
      "Epoch 87/300\n",
      "200/200 [==============================] - 80s 402ms/step - loss: 0.2215 - acc: 0.9144 - val_loss: 0.0206 - val_acc: 0.7744\n",
      "Epoch 88/300\n",
      "200/200 [==============================] - 84s 420ms/step - loss: 0.2174 - acc: 0.9187 - val_loss: 0.0183 - val_acc: 0.6087\n",
      "Epoch 89/300\n",
      "200/200 [==============================] - 84s 419ms/step - loss: 0.1947 - acc: 0.9306 - val_loss: 0.0382 - val_acc: 0.9178\n",
      "Epoch 90/300\n",
      "200/200 [==============================] - 77s 383ms/step - loss: 0.1915 - acc: 0.9317 - val_loss: 0.1959 - val_acc: 0.9519\n",
      "Epoch 91/300\n",
      "200/200 [==============================] - 76s 380ms/step - loss: 0.2151 - acc: 0.9211 - val_loss: 4.4940 - val_acc: 0.5906\n",
      "Epoch 92/300\n",
      "200/200 [==============================] - 82s 411ms/step - loss: 0.1807 - acc: 0.9364 - val_loss: 0.1898 - val_acc: 0.9573\n",
      "Epoch 93/300\n",
      "200/200 [==============================] - 84s 418ms/step - loss: 0.1825 - acc: 0.9339 - val_loss: 4.9419 - val_acc: 0.5958\n",
      "Epoch 94/300\n",
      "200/200 [==============================] - 82s 409ms/step - loss: 0.1806 - acc: 0.9351 - val_loss: 1.3986 - val_acc: 0.8610\n",
      "Epoch 95/300\n",
      "200/200 [==============================] - 77s 387ms/step - loss: 0.1857 - acc: 0.9348 - val_loss: 1.5083 - val_acc: 0.8406\n",
      "Epoch 96/300\n",
      "200/200 [==============================] - 84s 421ms/step - loss: 0.1818 - acc: 0.9279 - val_loss: 0.1731 - val_acc: 0.9468\n",
      "Epoch 97/300\n",
      "200/200 [==============================] - 84s 419ms/step - loss: 0.1833 - acc: 0.9347 - val_loss: 0.0170 - val_acc: 0.6876\n",
      "Epoch 98/300\n",
      "200/200 [==============================] - 82s 412ms/step - loss: 0.1684 - acc: 0.9421 - val_loss: 2.4483 - val_acc: 0.7311\n",
      "Epoch 99/300\n",
      "200/200 [==============================] - 77s 387ms/step - loss: 0.1676 - acc: 0.9421 - val_loss: 1.6586 - val_acc: 0.8239\n",
      "Epoch 100/300\n",
      "200/200 [==============================] - 83s 417ms/step - loss: 0.1559 - acc: 0.9449 - val_loss: 0.3101 - val_acc: 0.9384\n",
      "Epoch 101/300\n",
      "200/200 [==============================] - 84s 419ms/step - loss: 0.1683 - acc: 0.9416 - val_loss: 0.0618 - val_acc: 0.9301\n",
      "Epoch 102/300\n",
      "200/200 [==============================] - 82s 412ms/step - loss: 0.1833 - acc: 0.9332 - val_loss: 1.2273 - val_acc: 0.8795\n",
      "Epoch 103/300\n",
      "200/200 [==============================] - 76s 379ms/step - loss: 0.1736 - acc: 0.9364 - val_loss: 2.4043 - val_acc: 0.7663\n",
      "Epoch 104/300\n",
      "200/200 [==============================] - 83s 415ms/step - loss: 0.1594 - acc: 0.9436 - val_loss: 4.3782 - val_acc: 0.6025\n",
      "Epoch 105/300\n",
      "200/200 [==============================] - 83s 413ms/step - loss: 0.1626 - acc: 0.9419 - val_loss: 0.0391 - val_acc: 0.9496\n",
      "Epoch 106/300\n",
      "200/200 [==============================] - 82s 410ms/step - loss: 0.1517 - acc: 0.9463 - val_loss: 1.1885 - val_acc: 0.8902\n",
      "Epoch 107/300\n",
      "200/200 [==============================] - 76s 380ms/step - loss: 0.1525 - acc: 0.9449 - val_loss: 3.2084 - val_acc: 0.6559\n",
      "Epoch 108/300\n",
      "200/200 [==============================] - 79s 395ms/step - loss: 0.1541 - acc: 0.9443 - val_loss: 1.2162 - val_acc: 0.8937\n",
      "Epoch 109/300\n",
      "200/200 [==============================] - 83s 414ms/step - loss: 0.1439 - acc: 0.9539 - val_loss: 0.0266 - val_acc: 0.7283\n",
      "Epoch 110/300\n",
      "200/200 [==============================] - 82s 412ms/step - loss: 0.1491 - acc: 0.9504 - val_loss: 0.0164 - val_acc: 0.7746\n",
      "Epoch 111/300\n",
      "200/200 [==============================] - 79s 394ms/step - loss: 0.1468 - acc: 0.9467 - val_loss: 0.0340 - val_acc: 0.9359\n",
      "Epoch 112/300\n",
      "200/200 [==============================] - 77s 387ms/step - loss: 0.1471 - acc: 0.9480 - val_loss: 2.1053 - val_acc: 0.7718\n",
      "Epoch 113/300\n",
      "200/200 [==============================] - 83s 414ms/step - loss: 0.1406 - acc: 0.9526 - val_loss: 3.5738 - val_acc: 0.6366\n",
      "Epoch 114/300\n",
      "200/200 [==============================] - 80s 399ms/step - loss: 0.1495 - acc: 0.9490 - val_loss: 0.0180 - val_acc: 0.7261\n",
      "Epoch 115/300\n",
      "200/200 [==============================] - 83s 413ms/step - loss: 0.1450 - acc: 0.9510 - val_loss: 0.5249 - val_acc: 0.9411\n",
      "Epoch 116/300\n",
      "200/200 [==============================] - 78s 388ms/step - loss: 0.1619 - acc: 0.9430 - val_loss: 2.6318 - val_acc: 0.7182\n",
      "Epoch 117/300\n",
      "200/200 [==============================] - 84s 418ms/step - loss: 0.1501 - acc: 0.9487 - val_loss: 0.0432 - val_acc: 0.9624\n",
      "Epoch 118/300\n",
      "200/200 [==============================] - 83s 416ms/step - loss: 0.1375 - acc: 0.9559 - val_loss: 0.2283 - val_acc: 0.9636\n",
      "Epoch 119/300\n",
      "200/200 [==============================] - 83s 416ms/step - loss: 0.1462 - acc: 0.9511 - val_loss: 1.6863 - val_acc: 0.8363\n",
      "Epoch 120/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 76s 379ms/step - loss: 0.1475 - acc: 0.9477 - val_loss: 2.6177 - val_acc: 0.7278\n",
      "Epoch 121/300\n",
      "200/200 [==============================] - 84s 419ms/step - loss: 0.1400 - acc: 0.9503 - val_loss: 3.0437 - val_acc: 0.6697\n",
      "Epoch 122/300\n",
      "200/200 [==============================] - 83s 413ms/step - loss: 0.1444 - acc: 0.9484 - val_loss: 0.0161 - val_acc: 0.6908\n",
      "Epoch 123/300\n",
      "200/200 [==============================] - 84s 419ms/step - loss: 0.1514 - acc: 0.9504 - val_loss: 0.2615 - val_acc: 0.9078\n",
      "Epoch 124/300\n",
      "200/200 [==============================] - 76s 382ms/step - loss: 0.1373 - acc: 0.9525 - val_loss: 3.5985 - val_acc: 0.6674\n",
      "Epoch 125/300\n",
      "200/200 [==============================] - 83s 414ms/step - loss: 0.1384 - acc: 0.9556 - val_loss: 0.8926 - val_acc: 0.9307\n",
      "Epoch 126/300\n",
      "200/200 [==============================] - 84s 418ms/step - loss: 0.1420 - acc: 0.9520 - val_loss: 0.6931 - val_acc: 0.9173\n",
      "Epoch 127/300\n",
      "200/200 [==============================] - 84s 419ms/step - loss: 0.1403 - acc: 0.9515 - val_loss: 0.5710 - val_acc: 0.9486\n",
      "Epoch 128/300\n",
      "200/200 [==============================] - 76s 380ms/step - loss: 0.1314 - acc: 0.9561 - val_loss: 2.1239 - val_acc: 0.7882\n",
      "Epoch 129/300\n",
      "200/200 [==============================] - 80s 399ms/step - loss: 0.1360 - acc: 0.9554 - val_loss: 0.1899 - val_acc: 0.9693\n",
      "Epoch 130/300\n",
      "200/200 [==============================] - 84s 419ms/step - loss: 0.1271 - acc: 0.9558 - val_loss: 2.2667 - val_acc: 0.7454\n",
      "Epoch 131/300\n",
      "200/200 [==============================] - 84s 420ms/step - loss: 0.1220 - acc: 0.9608 - val_loss: 2.9321 - val_acc: 0.7000\n",
      "Epoch 132/300\n",
      "200/200 [==============================] - 77s 387ms/step - loss: 0.1270 - acc: 0.9592 - val_loss: 2.0622 - val_acc: 0.8137\n",
      "Epoch 133/300\n",
      "200/200 [==============================] - 77s 386ms/step - loss: 0.1269 - acc: 0.9556 - val_loss: 0.7865 - val_acc: 0.9439\n",
      "Epoch 134/300\n",
      "200/200 [==============================] - 83s 416ms/step - loss: 0.1298 - acc: 0.9568 - val_loss: 0.0636 - val_acc: 0.9628\n",
      "Epoch 135/300\n",
      "200/200 [==============================] - 84s 421ms/step - loss: 0.1300 - acc: 0.9572 - val_loss: 0.0238 - val_acc: 0.9511\n",
      "Epoch 136/300\n",
      "200/200 [==============================] - 82s 408ms/step - loss: 0.1290 - acc: 0.9565 - val_loss: 1.5448 - val_acc: 0.8603\n",
      "Epoch 137/300\n",
      "200/200 [==============================] - 74s 371ms/step - loss: 0.1275 - acc: 0.9577 - val_loss: 0.7357 - val_acc: 0.9463\n",
      "Epoch 138/300\n",
      "200/200 [==============================] - 83s 416ms/step - loss: 0.1161 - acc: 0.9597 - val_loss: 2.6237 - val_acc: 0.7283\n",
      "Epoch 139/300\n",
      "200/200 [==============================] - 84s 418ms/step - loss: 0.1240 - acc: 0.9572 - val_loss: 4.0527 - val_acc: 0.6129\n",
      "Epoch 140/300\n",
      "200/200 [==============================] - 81s 405ms/step - loss: 0.1144 - acc: 0.9632 - val_loss: 1.8138 - val_acc: 0.8205\n",
      "Epoch 141/300\n",
      "200/200 [==============================] - 76s 380ms/step - loss: 0.1212 - acc: 0.9602 - val_loss: 0.2465 - val_acc: 0.9593\n",
      "Epoch 142/300\n",
      "200/200 [==============================] - 83s 413ms/step - loss: 0.1135 - acc: 0.9639 - val_loss: 1.7994 - val_acc: 0.7987\n",
      "Epoch 143/300\n",
      "200/200 [==============================] - 83s 414ms/step - loss: 0.1217 - acc: 0.9592 - val_loss: 0.0155 - val_acc: 0.5529\n",
      "Epoch 144/300\n",
      "200/200 [==============================] - 82s 412ms/step - loss: 0.1202 - acc: 0.9605 - val_loss: 1.9094 - val_acc: 0.8080\n",
      "Epoch 145/300\n",
      "200/200 [==============================] - 76s 380ms/step - loss: 0.1142 - acc: 0.9617 - val_loss: 1.1987 - val_acc: 0.8769\n",
      "Epoch 146/300\n",
      "200/200 [==============================] - 83s 416ms/step - loss: 0.1141 - acc: 0.9658 - val_loss: 0.9410 - val_acc: 0.9293\n",
      "Epoch 147/300\n",
      "200/200 [==============================] - 84s 420ms/step - loss: 0.1109 - acc: 0.9638 - val_loss: 0.6527 - val_acc: 0.9416\n",
      "Epoch 148/300\n",
      "200/200 [==============================] - 80s 401ms/step - loss: 0.1185 - acc: 0.9608 - val_loss: 1.2826 - val_acc: 0.9114\n",
      "Epoch 149/300\n",
      "200/200 [==============================] - 75s 373ms/step - loss: 0.1145 - acc: 0.9635 - val_loss: 1.5265 - val_acc: 0.8716\n",
      "Epoch 150/300\n",
      "200/200 [==============================] - 76s 381ms/step - loss: 0.1128 - acc: 0.9636 - val_loss: 0.0405 - val_acc: 0.9598\n",
      "Epoch 151/300\n",
      "200/200 [==============================] - 83s 415ms/step - loss: 0.1045 - acc: 0.9662 - val_loss: 0.9725 - val_acc: 0.9000\n",
      "Epoch 152/300\n",
      "200/200 [==============================] - 83s 416ms/step - loss: 0.1070 - acc: 0.9642 - val_loss: 2.9632 - val_acc: 0.6978\n",
      "Epoch 153/300\n",
      "200/200 [==============================] - 82s 410ms/step - loss: 0.1258 - acc: 0.9586 - val_loss: 3.1446 - val_acc: 0.7347\n",
      "Epoch 154/300\n",
      "200/200 [==============================] - 77s 384ms/step - loss: 0.1117 - acc: 0.9620 - val_loss: 1.3369 - val_acc: 0.8791\n",
      "Epoch 155/300\n",
      "200/200 [==============================] - 83s 413ms/step - loss: 0.1112 - acc: 0.9633 - val_loss: 0.5592 - val_acc: 0.9617\n",
      "Epoch 156/300\n",
      "200/200 [==============================] - 83s 416ms/step - loss: 0.1231 - acc: 0.9610 - val_loss: 2.8454 - val_acc: 0.7904\n",
      "Epoch 157/300\n",
      "200/200 [==============================] - 83s 414ms/step - loss: 0.1145 - acc: 0.9626 - val_loss: 0.9146 - val_acc: 0.9265\n",
      "Epoch 158/300\n",
      "200/200 [==============================] - 77s 387ms/step - loss: 0.1129 - acc: 0.9617 - val_loss: 1.0843 - val_acc: 0.9060\n",
      "Epoch 159/300\n",
      "200/200 [==============================] - 82s 411ms/step - loss: 0.1066 - acc: 0.9664 - val_loss: 0.1421 - val_acc: 0.9549\n"
     ]
    }
   ],
   "source": [
    "# optimizer = SGD(lr=1e-3, momentum=0.9, nesterov=True)\n",
    "optimizer = Adam()\n",
    "model_top.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['acc'])\n",
    "callback_list = [EarlyStopping(monitor='val_acc', patience=30), \n",
    "                 ReduceLROnPlateau(monitor='loss', factor=np.sqrt(0.5), cooldown=0, patience=5, min_lr=0.5e-5)]\n",
    "output = model_top.fit_generator(ft_gen, steps_per_epoch=200, epochs=300,\n",
    "                                  validation_data=validation_generator, validation_steps=len(validation_generator), callbacks=callback_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 625/625 [02:18<00:00,  4.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 1 1 1]\n",
      "[0 0 0 ... 1 1 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "output_score50 = []\n",
    "output_class50 = []\n",
    "answer_class50 = []\n",
    "answer_class50_1 =[]\n",
    "\n",
    "for i in trange(len(test50_generator)):\n",
    "    output50 = model_top.predict_on_batch(test50_generator[i][0])\n",
    "    output_score50.append(output50)\n",
    "    answer_class50.append(test50_generator[i][1])\n",
    "    \n",
    "output_score50 = np.concatenate(output_score50)\n",
    "answer_class50 = np.concatenate(answer_class50)\n",
    "\n",
    "output_class50 = np.argmax(output_score50, axis=1)\n",
    "answer_class50_1 = np.argmax(answer_class50, axis=1)\n",
    "\n",
    "print(output_class50)\n",
    "print(answer_class50_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.98      0.96     10000\n",
      "           1       0.98      0.93      0.96     10000\n",
      "\n",
      "    accuracy                           0.96     20000\n",
      "   macro avg       0.96      0.96      0.96     20000\n",
      "weighted avg       0.96      0.96      0.96     20000\n",
      "\n",
      "[[9848  152]\n",
      " [ 689 9311]]\n",
      "AUROC: 0.993373\n",
      "0.19857373833312505\n",
      "test_acc:  0.95795\n"
     ]
    }
   ],
   "source": [
    "cm50 = confusion_matrix(answer_class50_1, output_class50)\n",
    "report50 = classification_report(answer_class50_1, output_class50)\n",
    "\n",
    "recall50 = cm50[0][0] / (cm50[0][0] + cm50[0][1])\n",
    "fallout50 = cm50[1][0] / (cm50[1][0] + cm50[1][1])\n",
    "\n",
    "fpr50, tpr50, thresholds50 = roc_curve(answer_class50_1, output_score50[:, 1], pos_label=1.)\n",
    "eer50 = brentq(lambda x : 1. - x - interp1d(fpr50, tpr50)(x), 0., 1.)\n",
    "thresh50 = interp1d(fpr50, thresholds50)(eer50)\n",
    "\n",
    "print(report50)\n",
    "print(cm50)\n",
    "print(\"AUROC: %f\" %(roc_auc_score(answer_class50_1, output_score50[:, 1])))\n",
    "print(thresh50)\n",
    "print('test_acc: ', len(output_class50[np.equal(output_class50, answer_class50_1)]) / len(output_class50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_top.save('/home/www/fake_detection/model/deepfake_shallownet_ft.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
